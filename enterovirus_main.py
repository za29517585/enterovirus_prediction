import pandas as pd
import numpy as np
import requests
import gspread
from datetime import datetime
from oauth2client.service_account import ServiceAccountCredentials
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# ==========================================
# 0. åƒæ•¸èˆ‡ç’°å¢ƒè¨­å®š
# ==========================================
TARGET_SHEET_URL = 'https://docs.google.com/spreadsheets/d/1seGpSiQSUCZMgEqs66nsycI5GLvqTiam8mLDry5G4t8/edit?usp=sharing'
SERVICE_ACCOUNT_FILE = 'service_account.json' 

# ==========================================
# 1. è³‡æ–™æŠ“å–æ¨¡çµ„
# ==========================================
def fetch_all_data():
    print("ğŸš€ æ­£åœ¨è¯ç¶²æŠ“å–æœ€æ–°è³‡æ–™...")
    # å¹¼å…’åœ’
    df_k = pd.read_csv("https://stats.moe.gov.tw/files/opendata/edu_B_1_4.csv", encoding='utf-8-sig')
    df_k = df_k[df_k['ç¸£å¸‚åˆ¥'] == 'è‡ºä¸­å¸‚'][['å­¸å¹´åº¦', 'å¹¼å…’åœ’[äºº]']]
    df_k['Year'] = df_k['å­¸å¹´åº¦'] + 1911
    df_k = df_k.rename(columns={'å¹¼å…’åœ’[äºº]': 'Kindergarten_Enrollment'})
    
    # ç–¾ç®¡ç½² - å¥ä¿
    df_nhi = pd.read_csv("https://od.cdc.gov.tw/eic/NHI_EnteroviralInfection.csv", encoding='utf-8-sig')
    df_nhi = df_nhi[(df_nhi['ç¸£å¸‚'] == 'å°ä¸­å¸‚') & (df_nhi['å¹´é½¡åˆ¥'].isin(['0~2', '3~6']))]
    df_nhi = df_nhi.groupby(['å¹´', 'é€±'])[['è…¸ç—…æ¯’å¥ä¿å°±è¨ºäººæ¬¡']].sum().reset_index()
    
    # ç–¾ç®¡ç½² - æ€¥è¨º (Target)
    df_er = pd.read_csv("https://od.cdc.gov.tw/eic/RODS_EnteroviralInfection.csv", encoding='utf-8-sig')
    df_er = df_er[(df_er['ç¸£å¸‚'] == 'å°ä¸­å¸‚') & (df_er['å¹´é½¡åˆ¥'].isin(['0', '1~3', '4~6']))]
    df_er = df_er.groupby(['å¹´', 'é€±'])[['è…¸ç—…æ¯’æ€¥è¨ºå°±è¨ºäººæ¬¡']].sum().reset_index()

    return df_er, df_nhi, df_k

# ==========================================
# 2. è³‡æ–™è™•ç†
# ==========================================
def process_data(df_er, df_nhi, df_k):
    df_er = df_er.rename(columns={'å¹´': 'Year', 'é€±': 'Week', 'è…¸ç—…æ¯’æ€¥è¨ºå°±è¨ºäººæ¬¡': 'EV_ER_Cases'})
    df_nhi = df_nhi.rename(columns={'å¹´': 'Year', 'é€±': 'Week', 'è…¸ç—…æ¯’å¥ä¿å°±è¨ºäººæ¬¡': 'EV_NHI_Cases'})
    
    df = pd.merge(df_er, df_nhi, on=['Year', 'Week'], how='left')
    df = pd.merge(df, df_k[['Year', 'Kindergarten_Enrollment']], on='Year', how='left')
    df = df.sort_values(['Year', 'Week']).reset_index(drop=True)
    
    # Lag 3 é‚è¼¯ (é æ¸¬ä¸‹é€±æ™‚ï¼Œä½¿ç”¨å…©é€±å‰è³‡æ–™)
    df['Lag3_ER'] = df['EV_ER_Cases'].shift(3)
    df['Lag4_ER'] = df['EV_ER_Cases'].shift(4)
    df['Lag3_NHI'] = df['EV_NHI_Cases'].shift(3)
    df['Kindergarten_Enrollment'] = df['Kindergarten_Enrollment'].ffill()
    
    df['Week_Sin'] = np.sin(2 * np.pi * df['Week'] / 53)
    df['Week_Cos'] = np.cos(2 * np.pi * df['Week'] / 53)
    return df

# ==========================================
# 3. æ¨¡å‹é æ¸¬æ ¸å¿ƒ
# ==========================================
def run_model_pipeline(df):
    features = ['Year', 'Week', 'Lag3_ER', 'Lag4_ER', 'Lag3_NHI', 'Kindergarten_Enrollment', 'Week_Sin', 'Week_Cos']
    target = 'EV_ER_Cases'
    
    train_df = df.dropna(subset=features + [target])
    model = RandomForestRegressor(n_estimators=100, random_state=42)
    model.fit(train_df[features], train_df[target])
    
    mae = round(mean_absolute_error(train_df[target], model.predict(train_df[features])), 2)
    
    # é æ¸¬ä¸‹é€±
    now = datetime.now()
    _, cur_w, _ = now.isocalendar()
    t_year, t_week = (now.year, cur_w + 1) if cur_w < 53 else (now.year + 1, 1)
    
    latest = df.iloc[-1]
    input_v = pd.DataFrame([{
        'Year': t_year, 'Week': t_week,
        'Lag3_ER': latest['EV_ER_Cases'], 'Lag4_ER': df.iloc[-2]['EV_ER_Cases'],
        'Lag3_NHI': latest['EV_NHI_Cases'], 'Kindergarten_Enrollment': latest['Kindergarten_Enrollment'],
        'Week_Sin': np.sin(2 * np.pi * t_week / 53), 'Week_Cos': np.cos(2 * np.pi * t_week / 53)
    }])
    
    prediction = model.predict(input_v)[0]
    
    # å»ºç«‹è¼¸å‡ºçµæœ
    pred_res = pd.DataFrame([{
        'Forecast_Timestamp': now.strftime('%Y-%m-%d %H:%M'),
        'Target_Period': f"{int(t_year)}W{int(t_week):02d}",
        'Predicted_ER_Cases': round(prediction, 2),
        'Model_MAE': mae,
        'Input_Ref_Week': f"{int(latest['Year'])}W{int(latest['Week']):02d}",
        'Ref_Actual_ER': latest['EV_ER_Cases'],
        'Ref_Actual_NHI': latest['EV_NHI_Cases']
    }])
    
    importances = pd.DataFrame({
        'Feature': features,
        'Importance': model.feature_importances_
    }).sort_values(by='Importance', ascending=False)
    
    return pred_res, importances

# ==========================================
# 4. Google Sheets ä¸Šå‚³æ¨¡çµ„ (ä¿®æ­£æ¬„ä½åç¨±éºå¤±å•é¡Œ)
# ==========================================
def upload_to_sheets(pred_df, importance_df):
    print("ğŸ“¤ æ­£åœ¨åŒæ­¥è³‡æ–™è‡³ Google Sheets...")
    scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
    creds = ServiceAccountCredentials.from_json_keyfile_name(SERVICE_ACCOUNT_FILE, scope)
    client = gspread.authorize(creds)
    sheet = client.open_by_url(TARGET_SHEET_URL)
    
    # --- 1. è™•ç†ã€Œé æ¸¬çµæœã€åˆ†é  ---
    try:
        ws_pred = sheet.worksheet("é æ¸¬çµæœ")
    except:
        ws_pred = sheet.add_worksheet(title="é æ¸¬çµæœ", rows="100", cols="10")
    
    # æª¢æŸ¥æ˜¯å¦æœ‰æ¨™é¡Œåˆ—ï¼Œè‹¥ç„¡æˆ–ä¸å°å‰‡å¯«å…¥
    headers = pred_df.columns.tolist()
    current_values = ws_pred.get_all_values()
    
    if not current_values or current_values[0] != headers:
        # å¦‚æœæ˜¯ç©ºçš„æˆ–æ¨™é¡Œä¸å°ï¼Œåœ¨ç¬¬ä¸€åˆ—æ’å…¥æ¨™é¡Œ
        ws_pred.insert_row(headers, 1)
        print("ğŸ’¡ å·²è‡ªå‹•å»ºç«‹/æ ¡æ­£æ¨™é¡Œåˆ—ã€‚")
    
    # é™„åŠ æ•¸æ“š
    ws_pred.append_rows(pred_df.values.tolist())

    # --- 2. è™•ç†ã€Œæ¨¡å‹ç›£æ§ã€åˆ†é  ---
    try:
        ws_stats = sheet.worksheet("æ¨¡å‹ç›£æ§")
    except:
        ws_stats = sheet.add_worksheet(title="æ¨¡å‹ç›£æ§", rows="100", cols="10")
    
    ws_stats.clear()
    # å¯«å…¥ç‰¹å¾µé‡è¦æ€§èˆ‡æ¨™é¡Œ
    ws_stats.update('A1', [['æ¨¡å‹ç‰¹å¾µé‡è¦æ€§åˆ†æ (Feature Importance)']])
    ws_stats.update('A2', [importance_df.columns.tolist()]) # æ¬„ä½æ¨™é¡Œ
    ws_stats.update('A3', importance_df.values.tolist()) # æ•¸å€¼å…§å®¹
    
    print("âœ… Sheets æ›´æ–°æˆåŠŸï¼")

# ==========================================
# 5. ä¸»ç¨‹å¼åŸ·è¡Œ
# ==========================================
if __name__ == "__main__":
    try:
        er, nhi, k = fetch_all_data()
        df_final = process_data(er, nhi, k)
        p_res, f_imp = run_model_pipeline(df_final)
        upload_to_sheets(p_res, f_imp)
        print(f"\nğŸ‰ åŸ·è¡ŒæˆåŠŸï¼ä¸‹é€±é æ¸¬å€¼ç‚º: {p_res['Predicted_ER_Cases'].iloc[0]}")
    except Exception as e:
        print(f"âŒ éŒ¯èª¤: {e}")