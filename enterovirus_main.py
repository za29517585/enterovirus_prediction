import requests
import pandas as pd
import numpy as np
import os
import gspread
from datetime import datetime, timedelta
from oauth2client.service_account import ServiceAccountCredentials
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error

# ==========================================
# 0. åƒæ•¸èˆ‡ç’°å¢ƒè¨­å®š
# ==========================================
TARGET_SHEET_URL = 'https://docs.google.com/spreadsheets/d/1seGpSiQSUCZMgEqs66nsycI5GLvqTiam8mLDry5G4t8/edit?usp=sharing'
SERVICE_ACCOUNT_FILE = 'service_account.json'
gcp_json_content = os.getenv("GCP_SA_KEY")

if gcp_json_content:
    print("âœ… åµæ¸¬åˆ° GCP_SERVICE_ACCOUNT ç’°å¢ƒè®Šæ•¸ï¼Œæ­£åœ¨ç”¢ç”Ÿæ†‘è­‰æª”...")
    with open(SERVICE_ACCOUNT_FILE, 'w') as f:
        f.write(gcp_json_content)
else:
    print("âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° GCP_SERVICE_ACCOUNT ç’°å¢ƒè®Šæ•¸ï¼Œè«‹æª¢æŸ¥ GitHub Secrets è¨­å®šã€‚")
    # å¦‚æœæ˜¯åœ¨æœ¬åœ°æ¸¬è©¦ï¼Œä¸”ä½ æœ‰æª”æ¡ˆçš„è©±ï¼Œå¯ä»¥ä¸å ±éŒ¯ï¼›
    # ä½†åœ¨ GitHub Actions ä¸Šé€™æœƒå°è‡´å¾ŒçºŒ upload å¤±æ•—ã€‚

CWA_API_URL = "https://opendata.cwa.gov.tw/api/v1/rest/datastore/C-B0024-001"
CWA_TOKEN = os.getenv("CWA_TOKEN")
PM25_API_URL = "https://data.moenv.gov.tw/api/v2/aqx_p_322?api_key=4c89a32a-a214-461b-bf29-30ff32a61a8a&sort=monitordate%20desc&format=CSV"

HIST_DIR = "./hist_data/"

# ==========================================
# 1. è¼”åŠ©å‡½å¼ï¼šè®€å–æˆ–æŠ“å–è³‡æ–™
# ==========================================

def get_historical_or_fetch_new(file_name, fetch_func):
    """å˜—è©¦è®€å–æ­·å²æª”ï¼Œä¸¦åŸ·è¡Œçˆ¬èŸ²æŠ“å–æœ€æ–°è³‡æ–™"""
    file_path = os.path.join(HIST_DIR, file_name)
    
    # åŸ·è¡Œçˆ¬èŸ²ç²å–æœ€æ–°é€±è³‡æ–™ (é€™éƒ¨åˆ†æ²¿ç”¨æ‚¨ä¹‹å‰çš„ ETL é‚è¼¯)
    fetch_func() 
    
    if os.path.exists(file_path):
        return pd.read_csv(file_path)
    else:
        print(f"âš ï¸ æ‰¾ä¸åˆ°æ­·å²æª”æ¡ˆ: {file_name}")
        return pd.DataFrame()

def fetch_all_source_data():
    print("ğŸš€ æ­£åœ¨åŒæ­¥æ‰€æœ‰ä¾†æºè³‡æ–™ (CDC, MOE, CWA, MoENV)...")
    
    # A. å¹¼å…’åœ’äººæ•¸
    df_k = pd.read_csv("https://stats.moe.gov.tw/files/opendata/edu_B_1_4.csv", encoding='utf-8-sig')
    df_k = df_k[df_k['ç¸£å¸‚åˆ¥'] == 'è‡ºä¸­å¸‚'][['å­¸å¹´åº¦', 'å¹¼å…’åœ’[äºº]']]
    df_k['Year'] = df_k['å­¸å¹´åº¦'] + 1911
    df_k = df_k.rename(columns={'å¹¼å…’åœ’[äºº]': 'Kindergarten_Enrollment'})

    # B. CDC è…¸ç—…æ¯’è³‡æ–™
    df_nhi = pd.read_csv("https://od.cdc.gov.tw/eic/NHI_EnteroviralInfection.csv", encoding='utf-8-sig')
    df_nhi = df_nhi[(df_nhi['ç¸£å¸‚'] == 'å°ä¸­å¸‚') & (df_nhi['å¹´é½¡åˆ¥'].isin(['0~2', '3~6']))]
    df_nhi = df_nhi.groupby(['å¹´', 'é€±'])[['è…¸ç—…æ¯’å¥ä¿å°±è¨ºäººæ¬¡']].sum().reset_index()
    
    df_er = pd.read_csv("https://od.cdc.gov.tw/eic/RODS_EnteroviralInfection.csv", encoding='utf-8-sig')
    df_er = df_er[(df_er['ç¸£å¸‚'] == 'å°ä¸­å¸‚') & (df_er['å¹´é½¡åˆ¥'].isin(['0', '1~3', '4~6']))]
    df_er = df_er.groupby(['å¹´', 'é€±'])[['è…¸ç—…æ¯’æ€¥è¨ºå°±è¨ºäººæ¬¡']].sum().reset_index()

    # C. è®€å–æ°£è±¡èˆ‡ PM2.5 æ­·å²å­˜æª” (å‡è¨­æ‚¨ä¹‹å‰çš„ ETL å·²ç¶“è·‘éä¸¦å­˜æª”)
    df_temp = pd.read_csv(os.path.join(HIST_DIR, 'temp_hist.csv')) if os.path.exists(os.path.join(HIST_DIR, 'temp_hist.csv')) else pd.DataFrame()
    df_rh = pd.read_csv(os.path.join(HIST_DIR, 'rh_hist.csv')) if os.path.exists(os.path.join(HIST_DIR, 'rh_hist.csv')) else pd.DataFrame()
    df_pm = pd.read_csv(os.path.join(HIST_DIR, 'pm25_hist.csv')) if os.path.exists(os.path.join(HIST_DIR, 'pm25_hist.csv')) else pd.DataFrame()

    return df_er, df_nhi, df_k, df_temp, df_rh, df_pm

# ==========================================
# 2. è³‡æ–™è™•ç†
# ==========================================
def process_data(df_er, df_nhi, df_k, df_temp, df_rh, df_pm):
    print("ğŸ“Š æ•´åˆç‰¹å¾µèˆ‡æ­·å²è³‡æ–™...")
    
    # æ¨™æº–åŒ–æ¬„ä½åç¨±ä»¥åˆ©åˆä½µ
    df_er = df_er.rename(columns={'å¹´': 'Year', 'é€±': 'Week', 'è…¸ç—…æ¯’æ€¥è¨ºå°±è¨ºäººæ¬¡': 'EV_ER'})
    df_nhi = df_nhi.rename(columns={'å¹´': 'Year', 'é€±': 'Week', 'è…¸ç—…æ¯’å¥ä¿å°±è¨ºäººæ¬¡': 'EV_NHI'})
    
    # åˆä½µ CDC è³‡æ–™
    df = pd.merge(df_er, df_nhi, on=['Year', 'Week'], how='outer')
    df['EV_Total_Cases'] = df['EV_ER'].fillna(0) + df['EV_NHI'].fillna(0)
    
    # åˆä½µæ­·å²æ°£è±¡èˆ‡ PM2.5 (é€™æ­¥æœ€é—œéµï¼Œæ±ºå®šäº†æ¨¡å‹æœ‰æ²’æœ‰è¨“ç·´æ¨£æœ¬)
    if not df_temp.empty:
        df_temp = df_temp.rename(columns={'å¹´': 'Year', 'é€±æ¬¡': 'Week', 'è‡ºä¸­å¸‚æ°£æº«_é€±å¹³å‡': 'temp'})
        df = pd.merge(df, df_temp, on=['Year', 'Week'], how='left')
    
    if not df_rh.empty:
        df_rh = df_rh.rename(columns={'å¹´': 'Year', 'é€±æ¬¡': 'Week', 'è‡ºä¸­å¸‚ç›¸å°æº¼åº¦_é€±å¹³å‡': 'rh'})
        df = pd.merge(df, df_rh, on=['Year', 'Week'], how='left')
        
    if not df_pm.empty:
        df_pm = df_pm.rename(columns={'å¹´': 'Year', 'é€±æ¬¡': 'Week', 'è‡ºä¸­å¸‚PM2.5_é€±å¹³å‡': 'PM25'})
        df = pd.merge(df, df_pm, on=['Year', 'Week'], how='left')

    df = pd.merge(df, df_k[['Year', 'Kindergarten_Enrollment']], on='Year', how='left')
    
    # æ’åºä¸¦è™•ç†ç‰¹å¾µ
    df = df.sort_values(['Year', 'Week']).reset_index(drop=True)
    df['Lag3_Total'] = df['EV_Total_Cases'].shift(3)
    df['Lag4_Total'] = df['EV_Total_Cases'].shift(4)
    
    # å¡«å……æ°£è±¡ç¼ºå€¼ (é‡å°æœ€æ–°é‚„æ²’æ¹Šæ»¿ä¸€é€±çš„éƒ¨åˆ†)
    for col in ['temp', 'rh', 'PM25', 'Kindergarten_Enrollment']:
        df[col] = df[col].ffill()

    # é€±æœŸç‰¹å¾µ
    df['Week_Sin'] = np.sin(2 * np.pi * df['Week'] / 53)
    df['Week_Cos'] = np.cos(2 * np.pi * df['Week'] / 53)
    
    return df

# ==========================================
# 3. æ¨¡å‹è¨“ç·´èˆ‡é æ¸¬ (åŠ å…¥æª¢æŸ¥æ©Ÿåˆ¶)
# ==========================================
def run_model_pipeline(df):
    features = ['Year', 'Week', 'Lag3_Total', 'Lag4_Total', 'temp', 'rh', 'PM25', 'Kindergarten_Enrollment', 'Week_Sin', 'Week_Cos']
    target = 'EV_Total_Cases'
    
    # æª¢æŸ¥è¨“ç·´é›†æ˜¯å¦ç‚ºç©º
    train_df = df.dropna(subset=features + [target])
    
    if train_df.empty:
        raise ValueError("âŒ è¨“ç·´è³‡æ–™é›†ç‚ºç©ºï¼è«‹æª¢æŸ¥æ­·å² CSV æª” (temp_hist.csv ç­‰) æ˜¯å¦æ­£ç¢ºå­˜åœ¨æ–¼ ./hist/ è³‡æ–™å¤¾ä¸­ã€‚")

    print(f"ğŸ“ˆ è¨“ç·´æ¨£æœ¬æ•¸: {len(train_df)}")
    
    model = RandomForestRegressor(n_estimators=100, random_state=42)
    model.fit(train_df[features], train_df[target])
    
    mae = round(mean_absolute_error(train_df[target], model.predict(train_df[features])), 2)
    
    # é æ¸¬ä¸‹é€±
    now_taipei = datetime.now() + timedelta(hours=8)
    _, cur_w, _ = now_taipei.isocalendar()
    target_year, target_week = (now_taipei.year, cur_w + 1) if cur_w < 53 else (now_taipei.year + 1, 1)
    
    latest = df.iloc[-1]
    input_v = pd.DataFrame([{
        'Year': target_year, 'Week': target_week,
        'Lag3_Total': latest['EV_Total_Cases'], 
        'Lag4_Total': df.iloc[-2]['EV_Total_Cases'],
        'temp': latest['temp'], 'rh': latest['rh'], 'PM25': latest['PM25'],
        'Kindergarten_Enrollment': latest['Kindergarten_Enrollment'],
        'Week_Sin': np.sin(2 * np.pi * target_week / 53), 
        'Week_Cos': np.cos(2 * np.pi * target_week / 53)
    }])
    
    prediction = model.predict(input_v)[0]
    
    pred_res = pd.DataFrame([{
        'Forecast_Timestamp': now_taipei.strftime('%Y-%m-%d %H:%M'),
        'Target_Period': f"{int(target_year)}W{int(target_week):02d}",
        'Predicted_Total_Cases': round(prediction, 2),
        'Model_MAE': mae,
        'Input_Ref_Week': f"{int(latest['Year'])}W{int(latest['Week']):02d}",
        'Ref_Actual_Total': latest['EV_Total_Cases']
    }])
    
    importances = pd.DataFrame({'Feature': features, 'Importance': model.feature_importances_}).sort_values(by='Importance', ascending=False)
    
    return pred_res, importances

# ==========================================
# 4. Google Sheets ä¸Šå‚³
# ==========================================
def upload_to_sheets(pred_df, importance_df):
    print("ğŸ“¤ æ­£åœ¨åŒæ­¥è³‡æ–™è‡³ Google Sheets...")
    scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
    creds = ServiceAccountCredentials.from_json_keyfile_name(SERVICE_ACCOUNT_FILE, scope)
    client = gspread.authorize(creds)
    sheet = client.open_by_url(TARGET_SHEET_URL)
    
    # --- è™•ç†ã€Œé æ¸¬çµæœã€ ---
    try:
        ws_pred = sheet.worksheet("é æ¸¬çµæœ")
    except:
        ws_pred = sheet.add_worksheet(title="é æ¸¬çµæœ", rows="100", cols="10")
    
    headers = pred_df.columns.tolist()
    current_values = ws_pred.get_all_values()
    if not current_values or current_values[0] != headers:
        ws_pred.insert_row(headers, 1) # è‡ªå‹•æ’å…¥æ¨™é¡Œ
    
    ws_pred.append_rows(pred_df.values.tolist())

    # --- è™•ç†ã€Œæ¨¡å‹ç›£æ§ã€ ---
    try:
        ws_stats = sheet.worksheet("æ¨¡å‹ç›£æ§")
    except:
        ws_stats = sheet.add_worksheet(title="æ¨¡å‹ç›£æ§", rows="100", cols="10")
    
    ws_stats.clear()
    ws_stats.update('A1', [['è…¸ç—…æ¯’é æ¸¬æ¨¡å‹ - ç‰¹å¾µé‡è¦æ€§åˆ†æ']])
    ws_stats.update('A2', [importance_df.columns.tolist()]) # æ¬„ä½æ¨™é¡Œ
    ws_stats.update('A3', importance_df.values.tolist()) # å…§å®¹
    print("âœ… Sheets æ›´æ–°å®Œæˆï¼")

# ==========================================
# 5. ä¸»ç¨‹å¼
# ==========================================
if __name__ == "__main__":
    try:
        # 1. æŠ“å–æ‰€æœ‰è³‡æ–™ (åŒ…å«è®€å–æœ¬åœ°æ­·å² CSV)
        df_er, df_nhi, df_k, df_temp, df_rh, df_pm = fetch_all_source_data()
        
        # 2. æ•´åˆè³‡æ–™
        df_final = process_data(df_er, df_nhi, df_k, df_temp, df_rh, df_pm)
        
        # 3. åŸ·è¡Œæ¨¡å‹èˆ‡é æ¸¬
        p_res, f_imp = run_model_pipeline(df_final)
        
        # 4. ä¸Šå‚³ (éœ€ç¢ºä¿æœ‰ service_account.json)
        upload_to_sheets(p_res, f_imp)
        
        print(f"\nğŸ‰ ä»»å‹™åŸ·è¡ŒæˆåŠŸï¼é æ¸¬ {p_res['Target_Period'].iloc[0]} è…¸ç—…æ¯’ç¸½å°±è¨ºäººæ¬¡ç‚º {p_res['Predicted_Total_Cases'].iloc[0]} äºº")
        
    except Exception as e:
        print(f"âŒ ç™¼ç”ŸéŒ¯èª¤: {e}")
