import requests
import pandas as pd
import numpy as np
import json
import os
import gspread
from datetime import datetime, timedelta
from oauth2client.service_account import ServiceAccountCredentials
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error

# ==========================================
# 0. åƒæ•¸èˆ‡ç’°å¢ƒè¨­å®š
# ==========================================
CWA_API_URL = "https://opendata.cwa.gov.tw/api/v1/rest/datastore/C-B0024-001"
CWA_TOKEN = os.getenv("CWA_TOKEN")
PM25_API_URL = "https://data.moenv.gov.tw/api/v2/aqx_p_322?api_key=4c89a32a-a214-461b-bf29-30ff32a61a8a&sort=monitordate%20desc&format=CSV"
TARGET_SHEET_URL = "https://docs.google.com/spreadsheets/d/1seGpSiQSUCZMgEqs66nsycI5GLvqTiam8mLDry5G4t8/edit?usp=sharing"
# LINE æ©Ÿå™¨äººè¨­å®š
LINE_TOKEN = os.getenv("LINE_CHANNEL_ACCESS_TOKEN")
LINE_USER = os.getenv("LINE_USER_ID")
GITHUB_REPO_URL = "https://raw.githubusercontent.com/za29517585/enterovirus_prediction/main"

HIST_DIR = "./hist_data/"

# ==========================================
# 1. è¼”åŠ©å‡½å¼ï¼šè®€å–æˆ–æŠ“å–è³‡æ–™
# ==========================================

def get_historical_or_fetch_new(file_name, fetch_func):
    """å˜—è©¦è®€å–æ­·å²æª”ï¼Œä¸¦åŸ·è¡Œçˆ¬èŸ²æŠ“å–æœ€æ–°è³‡æ–™"""
    file_path = os.path.join(HIST_DIR, file_name)
    
    # åŸ·è¡Œçˆ¬èŸ²ç²å–æœ€æ–°é€±è³‡æ–™ (é€™éƒ¨åˆ†æ²¿ç”¨æ‚¨ä¹‹å‰çš„ ETL é‚è¼¯)
    fetch_func() 
    
    if os.path.exists(file_path):
        return pd.read_csv(file_path)
    else:
        print(f"âš ï¸ æ‰¾ä¸åˆ°æ­·å²æª”æ¡ˆ: {file_name}")
        return pd.DataFrame()

def fetch_all_source_data():
    print("ğŸš€ æ­£åœ¨åŒæ­¥æ‰€æœ‰ä¾†æºè³‡æ–™ (CDC, MOE, CWA, MoENV)...")
    
    # A. å¹¼å…’åœ’äººæ•¸
    df_k = pd.read_csv("https://stats.moe.gov.tw/files/opendata/edu_B_1_4.csv", encoding='utf-8-sig')
    df_k = df_k[df_k['ç¸£å¸‚åˆ¥'] == 'è‡ºä¸­å¸‚'][['å­¸å¹´åº¦', 'å¹¼å…’åœ’[äºº]']]
    df_k['Year'] = df_k['å­¸å¹´åº¦'] + 1911
    df_k = df_k.rename(columns={'å¹¼å…’åœ’[äºº]': 'Kindergarten_Enrollment'})

    # B. CDC è…¸ç—…æ¯’è³‡æ–™
    df_nhi = pd.read_csv("https://od.cdc.gov.tw/eic/NHI_EnteroviralInfection.csv", encoding='utf-8-sig')
    df_nhi = df_nhi[(df_nhi['ç¸£å¸‚'] == 'å°ä¸­å¸‚') & (df_nhi['å¹´é½¡åˆ¥'].isin(['0~2', '3~6']))]
    df_nhi = df_nhi.groupby(['å¹´', 'é€±'])[['è…¸ç—…æ¯’å¥ä¿å°±è¨ºäººæ¬¡']].sum().reset_index()
    
    df_er = pd.read_csv("https://od.cdc.gov.tw/eic/RODS_EnteroviralInfection.csv", encoding='utf-8-sig')
    df_er = df_er[(df_er['ç¸£å¸‚'] == 'å°ä¸­å¸‚') & (df_er['å¹´é½¡åˆ¥'].isin(['0', '1~3', '4~6']))]
    df_er = df_er.groupby(['å¹´', 'é€±'])[['è…¸ç—…æ¯’æ€¥è¨ºå°±è¨ºäººæ¬¡']].sum().reset_index()

    # C. è®€å–æ°£è±¡èˆ‡ PM2.5 æ­·å²å­˜æª” (å‡è¨­æ‚¨ä¹‹å‰çš„ ETL å·²ç¶“è·‘éä¸¦å­˜æª”)
    df_temp = pd.read_csv(os.path.join(HIST_DIR, 'temp_hist.csv')) if os.path.exists(os.path.join(HIST_DIR, 'temp_hist.csv')) else pd.DataFrame()
    df_rh = pd.read_csv(os.path.join(HIST_DIR, 'rh_hist.csv')) if os.path.exists(os.path.join(HIST_DIR, 'rh_hist.csv')) else pd.DataFrame()
    df_pm = pd.read_csv(os.path.join(HIST_DIR, 'pm25_hist.csv')) if os.path.exists(os.path.join(HIST_DIR, 'pm25_hist.csv')) else pd.DataFrame()

    return df_er, df_nhi, df_k, df_temp, df_rh, df_pm

# ==========================================
# 2. è³‡æ–™è™•ç†
# ==========================================
def process_data(df_er, df_nhi, df_k, df_temp, df_rh, df_pm):
    print("ğŸ“Š æ•´åˆç‰¹å¾µèˆ‡æ­·å²è³‡æ–™...")
    
    # æ¨™æº–åŒ–æ¬„ä½åç¨±ä»¥åˆ©åˆä½µ
    df_er = df_er.rename(columns={'å¹´': 'Year', 'é€±': 'Week', 'è…¸ç—…æ¯’æ€¥è¨ºå°±è¨ºäººæ¬¡': 'EV_ER'})
    df_nhi = df_nhi.rename(columns={'å¹´': 'Year', 'é€±': 'Week', 'è…¸ç—…æ¯’å¥ä¿å°±è¨ºäººæ¬¡': 'EV_NHI'})
    
    # åˆä½µ CDC è³‡æ–™
    df = pd.merge(df_er, df_nhi, on=['Year', 'Week'], how='outer')
    df['EV_Total_Cases'] = df['EV_ER'].fillna(0) + df['EV_NHI'].fillna(0)
    
    # åˆä½µæ­·å²æ°£è±¡èˆ‡ PM2.5 (é€™æ­¥æœ€é—œéµï¼Œæ±ºå®šäº†æ¨¡å‹æœ‰æ²’æœ‰è¨“ç·´æ¨£æœ¬)
    if not df_temp.empty:
        df_temp = df_temp.rename(columns={'å¹´': 'Year', 'é€±æ¬¡': 'Week', 'è‡ºä¸­å¸‚æ°£æº«_é€±å¹³å‡': 'temp'})
        df = pd.merge(df, df_temp, on=['Year', 'Week'], how='left')
    
    if not df_rh.empty:
        df_rh = df_rh.rename(columns={'å¹´': 'Year', 'é€±æ¬¡': 'Week', 'è‡ºä¸­å¸‚ç›¸å°æº¼åº¦_é€±å¹³å‡': 'rh'})
        df = pd.merge(df, df_rh, on=['Year', 'Week'], how='left')
        
    if not df_pm.empty:
        df_pm = df_pm.rename(columns={'å¹´': 'Year', 'é€±æ¬¡': 'Week', 'è‡ºä¸­å¸‚PM2.5_é€±å¹³å‡': 'PM25'})
        df = pd.merge(df, df_pm, on=['Year', 'Week'], how='left')

    df = pd.merge(df, df_k[['Year', 'Kindergarten_Enrollment']], on='Year', how='left')
    
    # æ’åºä¸¦è™•ç†ç‰¹å¾µ
    df = df.sort_values(['Year', 'Week']).reset_index(drop=True)
    df['Lag3_Total'] = df['EV_Total_Cases'].shift(3)
    df['Lag4_Total'] = df['EV_Total_Cases'].shift(4)
    
    # å¡«å……æ°£è±¡ç¼ºå€¼ (é‡å°æœ€æ–°é‚„æ²’æ¹Šæ»¿ä¸€é€±çš„éƒ¨åˆ†)
    for col in ['temp', 'rh', 'PM25', 'Kindergarten_Enrollment']:
        df[col] = df[col].ffill()

    # é€±æœŸç‰¹å¾µ
    df['Week_Sin'] = np.sin(2 * np.pi * df['Week'] / 53)
    df['Week_Cos'] = np.cos(2 * np.pi * df['Week'] / 53)
    
    return df

# ==========================================
# 3. æ¨¡å‹è¨“ç·´èˆ‡é æ¸¬ (åŠ å…¥æª¢æŸ¥æ©Ÿåˆ¶)
# ==========================================
def run_model_pipeline(df):
    features = ['Year', 'Week', 'Lag3_Total', 'Lag4_Total', 'temp', 'rh', 'PM25', 'Kindergarten_Enrollment', 'Week_Sin', 'Week_Cos']
    target = 'EV_Total_Cases'
    
    # æª¢æŸ¥è¨“ç·´é›†æ˜¯å¦ç‚ºç©º
    train_df = df.dropna(subset=features + [target])
    
    if train_df.empty:
        raise ValueError("âŒ è¨“ç·´è³‡æ–™é›†ç‚ºç©ºï¼è«‹æª¢æŸ¥æ­·å² CSV æª” (temp_hist.csv ç­‰) æ˜¯å¦æ­£ç¢ºå­˜åœ¨æ–¼ ./hist/ è³‡æ–™å¤¾ä¸­ã€‚")

    print(f"ğŸ“ˆ è¨“ç·´æ¨£æœ¬æ•¸: {len(train_df)}")
    
    model = RandomForestRegressor(n_estimators=100, random_state=42)
    model.fit(train_df[features], train_df[target])
    
    mae = round(mean_absolute_error(train_df[target], model.predict(train_df[features])), 2)
    
    # é æ¸¬ä¸‹é€±
    now_taipei = datetime.now() + timedelta(hours=8)
    _, cur_w, _ = now_taipei.isocalendar()
    target_year, target_week = (now_taipei.year, cur_w + 1) if cur_w < 53 else (now_taipei.year + 1, 1)
    
    latest = df.iloc[-1]
    input_v = pd.DataFrame([{
        'Year': target_year, 'Week': target_week,
        'Lag3_Total': latest['EV_Total_Cases'], 
        'Lag4_Total': df.iloc[-2]['EV_Total_Cases'],
        'temp': latest['temp'], 'rh': latest['rh'], 'PM25': latest['PM25'],
        'Kindergarten_Enrollment': latest['Kindergarten_Enrollment'],
        'Week_Sin': np.sin(2 * np.pi * target_week / 53), 
        'Week_Cos': np.cos(2 * np.pi * target_week / 53)
    }])
    
    prediction = model.predict(input_v)[0]
    
    pred_res = pd.DataFrame([{
        'Forecast_Timestamp': now_taipei.strftime('%Y-%m-%d %H:%M'),
        'Target_Period': f"{int(target_year)}W{int(target_week):02d}",
        'Predicted_Total_Cases': round(prediction, 2),
        'Model_MAE': mae,
        'Input_Ref_Week': f"{int(latest['Year'])}W{int(latest['Week']):02d}",
        'Ref_Actual_Total': latest['EV_Total_Cases']
    }])
    
    importances = pd.DataFrame({'Feature': features, 'Importance': model.feature_importances_}).sort_values(by='Importance', ascending=False)
    
    return pred_res, importances

# ==========================================
# æ–°å¢åŠŸèƒ½ï¼šLINE æ©Ÿå™¨äººæ¨é€é€šçŸ¥
# ==========================================
def send_line_notification(prediction_val):
    if not LINE_TOKEN or not LINE_USER:
        print("âš ï¸ æ‰¾ä¸åˆ° LINE Token æˆ– User IDï¼Œè·³éé€šçŸ¥ç™¼é€ã€‚")
        return

    url = "https://api.line.me/v2/bot/message/push"
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {LINE_TOKEN}"
    }

    # åˆ¤æ–·é¢¨éšªç­‰ç´šèˆ‡è¨Šæ¯
    if prediction_val < 1040:
        status = "ğŸŸ¢ ä½é¢¨éšªï¼ˆå®‰å…¨æœŸï¼‰"
        msg = f"{status}\nä¸‹é€±é ä¼°äººæ•¸ï¼š{prediction_val}\nä¸‹é€±ç‚ºä½é¢¨éšªæœŸï¼Œå»ºè­°ç¶­æŒä¸€èˆ¬æ´—æ‰‹è¡›ç”Ÿç¿’æ…£ï¼Œè½å¯¦æ­£ç¢ºæ´—æ‰‹5æ­¥é©Ÿï¼šã€Œæ¿•ã€æ“ï¼ˆè‡³å°‘20ç§’ï¼‰ã€æ²–ã€æ§ã€æ“¦ã€ï¼Œä»¥åŠé˜²æ²»è…¸ç—…æ¯’5å£è¨£ã€Œå‹¤æ´—æ‰‹ã€è¶³ç¡çœ ã€å¤šé‹å‹•ã€æ´—ç©å…·ã€åŠæ—©æ²»ç™‚ã€ã€‚"
        img_list = ["low1.jpg", "low2.jpg"]
    elif 1040 <= prediction_val <= 1300:
        status = "ğŸŸ¡ ä¸­é¢¨éšªï¼ˆè­¦è¦ºæœŸï¼‰"
        msg = f"{status}\nä¸‹é€±é ä¼°äººæ•¸ï¼š{prediction_val}\nä¸‹é€±ç‚ºä¸­é¢¨éšªæœŸï¼Œè«‹å„ä½å®¶é•·è¦å¤šæ³¨æ„è‡ªå·±å°±å­¸çš„å­©å­å€‘é«”æº«åŠå£è…”æœ‰ç„¡å‡ºç¾å°æ°´æ³¡ï¼Œè©²æé«˜è­¦è¦ºå›‰ï½"
        img_list = ["mid.jpg"]
    else:
        status = "ğŸ”´ é«˜é¢¨éšªï¼ˆæµè¡ŒæœŸ)"
        msg = f"{status}\nä¸‹é€±é ä¼°äººæ•¸ï¼š{prediction_val}\nä¸‹é€±ç‚ºé«˜é¢¨éšªæœŸï¼Œè«‹è¨˜å¾—åšå¥½å€‹äººé˜²è­·åŠåŠ å¼·ç’°å¢ƒæ¶ˆæ¯’ï¼ˆå¦‚ 500ppm æ¼‚ç™½æ°´ï¼‰ï¼Œè‹¥å°æœ‹å‹æœ‰ç”Ÿç—…ç¾è±¡ï¼Œè¨˜å¾—è¦è½å¯¦ã€Œç”Ÿç—…ä¸ä¸Šå­¸ã€ï¼Œä¿è­·è‡ªå·±ä¿è­·åˆ¥äººï½"
        img_list = ["high.jpg"]

    # å°è£è¨Šæ¯å…§å®¹
    messages = [{"type": "text", "text": msg}]
    
    # åŠ å…¥åœ–ç‰‡è¨Šæ¯ (LINE API é™åˆ¶å–®æ¬¡ Push æœ€å¤š 5 å‰‡è¨Šæ¯)
    for img_name in img_list:
        img_url = f"{GITHUB_REPO_URL}/{img_name}"
        messages.append({
            "type": "image",
            "originalContentUrl": img_url,
            "previewImageUrl": img_url
        })

    payload = {
        "to": LINE_USER,
        "messages": messages
    }

    res = requests.post(url, headers=headers, data=json.dumps(payload))
    if res.status_code == 200:
        print(f"âœ… LINE é€šçŸ¥å·²ç™¼é€ï¼š{status}")
    else:
        print(f"âŒ LINE é€šçŸ¥ç™¼é€å¤±æ•—: {res.text}")

# ==========================================
# 4. Google Sheets ä¸Šå‚³
# ==========================================
def upload_to_sheets(pred_df, importance_df):
    print("ğŸ“¤ æ­£åœ¨é€éç’°å¢ƒè®Šæ•¸åŒæ­¥è³‡æ–™è‡³ Google Sheets...")
    
    # 1. å¾ç’°å¢ƒè®Šæ•¸ç²å– JSON å­—ä¸²
    gcp_sa_key_str = os.getenv("GCP_SA_KEY")
    
    if not gcp_sa_key_str:
        raise ValueError("âŒ æ‰¾ä¸åˆ°ç’°å¢ƒè®Šæ•¸ GCP_SA_KEYï¼Œè«‹æª¢æŸ¥è¨­å®šã€‚")
    
    # 2. å°‡ JSON å­—ä¸²è§£æç‚º Dictionary
    info = json.loads(gcp_sa_key_str)
    
    # 3. ä½¿ç”¨ from_json_keyfile_dict é€²è¡Œé©—è­‰
    scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
    creds = ServiceAccountCredentials.from_json_keyfile_dict(info, scope)
    client = gspread.authorize(creds)
    
    sheet = client.open_by_url(TARGET_SHEET_URL)
    
    # --- è™•ç†ã€Œé æ¸¬çµæœã€ ---
    try:
        ws_pred = sheet.worksheet("é æ¸¬çµæœ")
    except:
        ws_pred = sheet.add_worksheet(title="é æ¸¬çµæœ", rows="100", cols="10")
    
    headers = pred_df.columns.tolist()
    current_values = ws_pred.get_all_values()
    if not current_values or current_values[0] != headers:
        ws_pred.insert_row(headers, 1)
    
    ws_pred.append_rows(pred_df.values.tolist())

    # --- è™•ç†ã€Œæ¨¡å‹ç›£æ§ã€ ---
    try:
        ws_stats = sheet.worksheet("æ¨¡å‹ç›£æ§")
    except:
        ws_stats = sheet.add_worksheet(title="æ¨¡å‹ç›£æ§", rows="100", cols="10")
    
    ws_stats.clear()
    ws_stats.update('A1', [['è…¸ç—…æ¯’é æ¸¬æ¨¡å‹ - ç‰¹å¾µé‡è¦æ€§åˆ†æ']])
    ws_stats.update('A2', [importance_df.columns.tolist()])
    ws_stats.update('A3', importance_df.values.tolist())
    print("âœ… Sheets æ›´æ–°å®Œæˆï¼")

# ==========================================
# 5. ä¸»ç¨‹å¼
# ==========================================
if __name__ == "__main__":
    try:
        # 1. æŠ“å–æ‰€æœ‰è³‡æ–™
        df_er, df_nhi, df_k, df_temp, df_rh, df_pm = fetch_all_source_data()
        # 2. æ•´åˆè³‡æ–™
        df_final = process_data(df_er, df_nhi, df_k, df_temp, df_rh, df_pm)
        # 3. åŸ·è¡Œæ¨¡å‹èˆ‡é æ¸¬
        p_res, f_imp = run_model_pipeline(df_final)
        # 4. ä¸Šå‚³ Google Sheets
        upload_to_sheets(p_res, f_imp)
        
        # --- åŸ·è¡Œæ–°åŠŸèƒ½ï¼šç™¼é€ LINE é€šçŸ¥ ---
        prediction_val = p_res['Predicted_Total_Cases'].iloc[0]
        send_line_notification(prediction_val)
        
        print(f"\nğŸ‰ ä»»å‹™åŸ·è¡ŒæˆåŠŸï¼é æ¸¬äººæ•¸ç‚º {prediction_val}")
        
    except Exception as e:
        print(f"âŒ ç™¼ç”ŸéŒ¯èª¤: {e}")