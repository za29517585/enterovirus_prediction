import pandas as pd
import numpy as np
import requests
import gspread
from datetime import datetime, timedelta
from oauth2client.service_account import ServiceAccountCredentials
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error

# ==========================================
# 0. åƒæ•¸èˆ‡ç’°å¢ƒè¨­å®š
# ==========================================
# Google Sheets é€£çµèˆ‡é‡‘é‘°æª”å
TARGET_SHEET_URL = 'https://docs.google.com/spreadsheets/d/1seGpSiQSUCZMgEqs66nsycI5GLvqTiam8mLDry5G4t8/edit?usp=sharing'
SERVICE_ACCOUNT_FILE = 'service_account.json' 

# ==========================================
# 1. è³‡æ–™æŠ“å–æ¨¡çµ„ (Crawling from APIs)
# ==========================================
def fetch_all_data():
    print("ğŸš€ æ­£åœ¨è¯ç¶²æŠ“å–æ•™è‚²éƒ¨ã€ç–¾ç®¡ç½²æœ€æ–°è³‡æ–™...")
    
    # A. å¹¼å…’åœ’äººæ•¸ (å¹´åº¦è³‡æ–™)
    df_k = pd.read_csv("https://stats.moe.gov.tw/files/opendata/edu_B_1_4.csv", encoding='utf-8-sig')
    df_k = df_k[df_k['ç¸£å¸‚åˆ¥'] == 'è‡ºä¸­å¸‚'][['å­¸å¹´åº¦', 'å¹¼å…’åœ’[äºº]']]
    df_k['Year'] = df_k['å­¸å¹´åº¦'] + 1911
    df_k = df_k.rename(columns={'å¹¼å…’åœ’[äºº]': 'Kindergarten_Enrollment'})
    
    # B. ç–¾ç®¡ç½² - å¥ä¿å°±è¨º (é€±è³‡æ–™)
    df_nhi = pd.read_csv("https://od.cdc.gov.tw/eic/NHI_EnteroviralInfection.csv", encoding='utf-8-sig')
    df_nhi = df_nhi[(df_nhi['ç¸£å¸‚'] == 'å°ä¸­å¸‚') & (df_nhi['å¹´é½¡åˆ¥'].isin(['0~2', '3~6']))]
    df_nhi = df_nhi.groupby(['å¹´', 'é€±'])[['è…¸ç—…æ¯’å¥ä¿å°±è¨ºäººæ¬¡']].sum().reset_index()
    
    # C. ç–¾ç®¡ç½² - æ€¥è¨ºå°±è¨º (ç›®æ¨™è³‡æ–™)
    df_er = pd.read_csv("https://od.cdc.gov.tw/eic/RODS_EnteroviralInfection.csv", encoding='utf-8-sig')
    df_er = df_er[(df_er['ç¸£å¸‚'] == 'å°ä¸­å¸‚') & (df_er['å¹´é½¡åˆ¥'].isin(['0', '1~3', '4~6']))]
    df_er = df_er.groupby(['å¹´', 'é€±'])[['è…¸ç—…æ¯’æ€¥è¨ºå°±è¨ºäººæ¬¡']].sum().reset_index()

    return df_er, df_nhi, df_k

# ==========================================
# 2. è³‡æ–™è™•ç†èˆ‡ç‰¹å¾µå·¥ç¨‹ (Processing)
# ==========================================
def process_data(df_er, df_nhi, df_k):
    print("ğŸ“Š æ­£åœ¨è½‰æ›æ¬„ä½ä¸¦å»ºç«‹ Lag ç‰¹å¾µ...")
    df_er = df_er.rename(columns={'å¹´': 'Year', 'é€±': 'Week', 'è…¸ç—…æ¯’æ€¥è¨ºå°±è¨ºäººæ¬¡': 'EV_ER_Cases'})
    df_nhi = df_nhi.rename(columns={'å¹´': 'Year', 'é€±': 'Week', 'è…¸ç—…æ¯’å¥ä¿å°±è¨ºäººæ¬¡': 'EV_NHI_Cases'})
    
    # åˆä½µè³‡æ–™è¡¨
    df = pd.merge(df_er, df_nhi, on=['Year', 'Week'], how='left')
    df = pd.merge(df, df_k[['Year', 'Kindergarten_Enrollment']], on='Year', how='left')
    df = df.sort_values(['Year', 'Week']).reset_index(drop=True)
    
    # å»ºç«‹ Lag 3 ç‰¹å¾µ (ç‚ºäº†è§£æ±ºé€±å…­åŸ·è¡Œæ™‚è³‡æ–™å°šæœªæ›´æ–°åˆ°æœ€æ–°ä¸€é€±çš„å•é¡Œ)
    df['Lag3_ER'] = df['EV_ER_Cases'].shift(3)
    df['Lag4_ER'] = df['EV_ER_Cases'].shift(4)
    df['Lag3_NHI'] = df['EV_NHI_Cases'].shift(3)
    df['Kindergarten_Enrollment'] = df['Kindergarten_Enrollment'].ffill()
    
    # é€±æœŸç‰¹å¾µ (Sin/Cos)
    df['Week_Sin'] = np.sin(2 * np.pi * df['Week'] / 53)
    df['Week_Cos'] = np.cos(2 * np.pi * df['Week'] / 53)
    return df

# ==========================================
# 3. æ¨¡å‹è¨“ç·´èˆ‡é æ¸¬æ ¸å¿ƒ
# ==========================================
def run_model_pipeline(df):
    features = ['Year', 'Week', 'Lag3_ER', 'Lag4_ER', 'Lag3_NHI', 'Kindergarten_Enrollment', 'Week_Sin', 'Week_Cos']
    target = 'EV_ER_Cases'
    
    # è¨“ç·´æ¨¡å‹
    train_df = df.dropna(subset=features + [target])
    model = RandomForestRegressor(n_estimators=100, random_state=42)
    model.fit(train_df[features], train_df[target])
    
    # è¨ˆç®—æˆæ•ˆ (MAE)
    mae = round(mean_absolute_error(train_df[target], model.predict(train_df[features])), 2)
    
    # --- å°åŒ—æ™‚å€æ ¡æ­£ (UTC+8) ---
    now_taipei = datetime.now() + timedelta(hours=8)
    _, cur_w, _ = now_taipei.isocalendar()
    
    # é æ¸¬ä¸‹é€± (T+1)
    target_year, target_week = (now_taipei.year, cur_w + 1) if cur_w < 53 else (now_taipei.year + 1, 1)
    
    # ä½¿ç”¨ç›®å‰èƒ½æ‹¿åˆ°çš„æœ€æ–°ä¸€ç­†è³‡æ–™ä½œç‚º Lag3 çš„è¼¸å…¥
    latest = df.iloc[-1]
    input_v = pd.DataFrame([{
        'Year': target_year, 'Week': target_week,
        'Lag3_ER': latest['EV_ER_Cases'], 
        'Lag4_ER': df.iloc[-2]['EV_ER_Cases'],
        'Lag3_NHI': latest['EV_NHI_Cases'], 
        'Kindergarten_Enrollment': latest['Kindergarten_Enrollment'],
        'Week_Sin': np.sin(2 * np.pi * target_week / 53), 
        'Week_Cos': np.cos(2 * np.pi * target_week / 53)
    }])
    
    prediction = model.predict(input_v)[0]
    
    # æº–å‚™ä¸Šå‚³çµæœ
    pred_res = pd.DataFrame([{
        'Forecast_Timestamp': now_taipei.strftime('%Y-%m-%d %H:%M'),
        'Target_Period': f"{int(target_year)}W{int(target_week):02d}",
        'Predicted_ER_Cases': round(prediction, 2),
        'Model_MAE': mae,
        'Input_Ref_Week': f"{int(latest['Year'])}W{int(latest['Week']):02d}",
        'Ref_Actual_ER': latest['EV_ER_Cases'],
        'Ref_Actual_NHI': latest['EV_NHI_Cases']
    }])
    
    # ç‰¹å¾µé‡è¦æ€§
    importances = pd.DataFrame({
        'Feature': features,
        'Importance': model.feature_importances_
    }).sort_values(by='Importance', ascending=False)
    
    return pred_res, importances

# ==========================================
# 4. Google Sheets ä¸Šå‚³æ¨¡çµ„ (å«æ¨™é¡Œè‡ªå‹•æ ¡æ­£)
# ==========================================
def upload_to_sheets(pred_df, importance_df):
    print("ğŸ“¤ æ­£åœ¨åŒæ­¥è³‡æ–™è‡³ Google Sheets...")
    scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
    creds = ServiceAccountCredentials.from_json_keyfile_name(SERVICE_ACCOUNT_FILE, scope)
    client = gspread.authorize(creds)
    sheet = client.open_by_url(TARGET_SHEET_URL)
    
    # --- è™•ç†ã€Œé æ¸¬çµæœã€ ---
    try:
        ws_pred = sheet.worksheet("é æ¸¬çµæœ")
    except:
        ws_pred = sheet.add_worksheet(title="é æ¸¬çµæœ", rows="100", cols="10")
    
    headers = pred_df.columns.tolist()
    current_values = ws_pred.get_all_values()
    if not current_values or current_values[0] != headers:
        ws_pred.insert_row(headers, 1) # è‡ªå‹•æ’å…¥æ¨™é¡Œ
    
    ws_pred.append_rows(pred_df.values.tolist())

    # --- è™•ç†ã€Œæ¨¡å‹ç›£æ§ã€ ---
    try:
        ws_stats = sheet.worksheet("æ¨¡å‹ç›£æ§")
    except:
        ws_stats = sheet.add_worksheet(title="æ¨¡å‹ç›£æ§", rows="100", cols="10")
    
    ws_stats.clear()
    ws_stats.update('A1', [['è…¸ç—…æ¯’é æ¸¬æ¨¡å‹ - ç‰¹å¾µé‡è¦æ€§åˆ†æ']])
    ws_stats.update('A2', [importance_df.columns.tolist()]) # æ¬„ä½æ¨™é¡Œ
    ws_stats.update('A3', importance_df.values.tolist()) # å…§å®¹
    print("âœ… Sheets æ›´æ–°å®Œæˆï¼")

# ==========================================
# 5. ä¸»ç¨‹å¼
# ==========================================
if __name__ == "__main__":
    try:
        er, nhi, k = fetch_all_data()
        df_final = process_data(er, nhi, k)
        p_res, f_imp = run_model_pipeline(df_final)
        upload_to_sheets(p_res, f_imp)
        print(f"\nğŸ‰ ä»»å‹™åŸ·è¡ŒæˆåŠŸï¼é æ¸¬ {p_res['Target_Period'].iloc[0]} ç‚º {p_res['Predicted_ER_Cases'].iloc[0]} äºº")
    except Exception as e:
        print(f"âŒ ç™¼ç”ŸéŒ¯èª¤: {e}")
